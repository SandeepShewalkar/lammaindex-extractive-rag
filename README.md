# LlamaIndex Extractive RAG (Local, Ollama-Based)

This project demonstrates a **local Retrieval-Augmented Generation (RAG)**
pipeline using **LlamaIndex** and **Ollama**, without relying on OpenAI
or any external APIs.

---

## ğŸ”¹ Features

- Fully local RAG setup
- PDF and document ingestion
- Vector search using local embeddings
- Answer generation using LLaMA 3
- No OpenAI API key required
- Works offline

---

## ğŸ”¹ Tech Stack

- **LlamaIndex** â€“ RAG framework
- **Ollama** â€“ Local LLM & embedding runtime
- **mxbai-embed-large** â€“ Embedding model
- **LLaMA 3** â€“ Generative LLM
- **Python 3.10+**

---

## ğŸ”¹ Project Structure

```text
llamaindex-extractive-rag/
â”‚
â”œâ”€â”€ data/                  # Input documents (PDFs, text files)
â”‚   â””â”€â”€ your_document.pdf
â”‚
â”œâ”€â”€ main.py                # RAG pipeline implementation
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project documentation
```

## ğŸ”¹ Setup Instructions
1ï¸âƒ£ Create virtual environment
python -m venv venv
source venv/bin/activate

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Install & start Ollama
ollama serve

4ï¸âƒ£ Pull required models
ollama pull llama3
ollama pull mxbai-embed-large

## ğŸ”¹ Running the Project
```
python main.py
```

## ğŸ”¹ How It Works

- Documents are loaded from the data/ directory

- Text is split and embedded using a local Ollama embedding model

- A vector index is created for similarity search

- Relevant chunks are retrieved for a query

- LLaMA 3 generates a response using retrieved context

## ğŸ”¹ Important Note on Answer Accuracy

This project uses a generative RAG approach.

âœ” Good for explanations and summaries

âŒ Not guaranteed to return verbatim text from PDFs

If you need exact text from documents:

Use a retriever-only approach instead of an LLM-based query engine.

## ğŸ”¹ Example Query
```
Question:
What do we mean by valuable?

Answer: <Generated response based on document context>

```
## ğŸ”¹ Use Cases

- Internal knowledge base search

- Document Q&A

- Policy or handbook analysis

- Learning RAG with local models